{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.pika_env import PikaEnv\n",
    "from environments.helper import open_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # environment\n",
    "        self.base_address = 0x0000179C\n",
    "        self.image_size = [84,84]\n",
    "        self.interval_time = 0.2\n",
    "        \n",
    "        # model\n",
    "        self.hidden_layer_size = 200\n",
    "        self.learning_rate = 0.0005\n",
    "        self.batch_size_episodes = 10\n",
    "        self.load_checkpoint = \"store_true\"\n",
    "        self.discount_factor = 0.99\n",
    "        self.render = \"store_true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found window : 0x002a0626: '뫮먰귃궔궭귙?갏?拒걍艱微뺂( 긹??붎 )'\n"
     ]
    }
   ],
   "source": [
    "env = PikaEnv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'뫮먰귃궔궭귙?갏?拒걍艱微뺂( 긹??붎 )'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action.window_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obervation, _, _, _ = env.step(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x84 at 0x21022F7EBA8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Image.fromarray(np.array(obervation[:, :, 2]))\n",
    "y = y.convert(\"RGB\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,3], dtype=tf.float32)\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-3d7d61c4ded1>:22: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d1a0c5cc1ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mepisodeBuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperience_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#Reset environment and get first new observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#s = processState(s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\pika_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\state.py\u001b[0m in \u001b[0;36mis_over\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mis_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\memory_reader.py\u001b[0m in \u001b[0;36mis_over\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# read flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 진행상태FLAG 0:(최초게임시작)공떨어지기전 1: 공떨어지기전 2:게임중 3:공이 땅에 닿임 4: 게임 종료 A: 메뉴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_process_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_address\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytes_read\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbyteorder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'little'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        #s = processState(s)\n",
    "        d = 0\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while True: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            #j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,13)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            s1,r,d, _ = env.step(a)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    print(\"UPDATING!\")\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == 4:\n",
    "                print(\"END ONE GAME\")\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
