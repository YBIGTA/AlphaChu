{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.pika_env import PikaEnv\n",
    "from environments.helper import open_game\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # environment\n",
    "        self.base_address = 0x00002A88\n",
    "        self.image_size = [84,84]\n",
    "        self.interval_time = 0.2\n",
    "        \n",
    "        # model\n",
    "        self.hidden_layer_size = 200\n",
    "        self.learning_rate = 0.0005\n",
    "        self.batch_size_episodes = 10\n",
    "        self.load_checkpoint = \"store_true\"\n",
    "        self.discount_factor = 0.99\n",
    "        self.render = \"store_true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found window : 0x000b04c0: '뫮먰귃궔궭귙?갏?拒걍艱微뺂( 긹??붎 )'\n"
     ]
    }
   ],
   "source": [
    "env = PikaEnv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'뫮먰귃궔궭귙?갏?拒걍艱微뺂( 긹??붎 )'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action.window_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observation = env.state.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256 at 0x23BB409C1D0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(observation).resize([256, 256], Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,3], dtype=tf.float32)\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 0 #How many steps of random actions before training begins.\n",
    "memory_steps = 0\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./dqn2\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(length, reward):\n",
    "    decay_factor = 0.99\n",
    "    return [decay_factor**(length-i-1)*reward for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn2\\model-80.ckpt\n",
      "Episode 0 Ended | Score 5 : 0\n",
      "Saved Model\n",
      "Episode 1 Ended | Score 5 : 0\n",
      "Episode 2 Ended | Score 5 : 0\n",
      "Episode 3 Ended | Score 5 : 0\n",
      "Episode 4 Ended | Score 5 : 0\n",
      "Episode 5 Ended | Score 5 : 0\n",
      "Episode 6 Ended | Score 5 : 0\n",
      "Episode 7 Ended | Score 5 : 0\n",
      "Episode 8 Ended | Score 5 : 0\n",
      "Episode 9 Ended | Score 5 : 0\n",
      "447 0.0 1\n",
      "Episode 10 Ended | Score 5 : 0\n",
      "Saved Model\n",
      "Episode 11 Ended | Score 5 : 0\n",
      "Episode 12 Ended | Score 5 : 0\n",
      "Episode 13 Ended | Score 5 : 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-3b8239d1beb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mtotal_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mroundBuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\pika_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, key_num)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# do action and get reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\action.py\u001b[0m in \u001b[0;36msend_key\u001b[1;34m(self, key_num)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \"\"\"\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Activate pika window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAppActivate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# 하나의 키일 때\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAppActivate\u001b[1;34m(self, *args)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "# myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        env.reset_game()\n",
    "        s, _, _, _ = env.step(0)\n",
    "        d = 0\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        # 1 Episode\n",
    "        while True:\n",
    "            roundBuffer = experience_buffer()\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,11)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            s1,r,d, _ = env.step(a)\n",
    "            total_steps += 1\n",
    "            roundBuffer.add(np.reshape(np.array([s,a,0,s1,d]),[1,5]))\n",
    "\n",
    "            if total_steps > pre_train_steps+memory_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:np.stack(trainBatch[:,0]),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "\n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            elif total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = memoryBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:np.stack(trainBatch[:,0]),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "\n",
    "                    updateTarget(targetOps,sess) \n",
    "            if d==3:\n",
    "                time.sleep(0.5)\n",
    "                reward = env.get_reward()\n",
    "                rAll += reward\n",
    "                buffer = roundBuffer.buffer\n",
    "                annealed_reward = smoothing(len(buffer), reward/2+len(buffer)/60)\n",
    "                for idx, x in enumerate(annealed_reward):\n",
    "                    buffer[idx][2] = x\n",
    "                episodeBuffer.add(buffer)\n",
    "                #continue\n",
    "\n",
    "            s = s1\n",
    "\n",
    "            if d==4:\n",
    "                com_score, my_score = env.state.get_score()\n",
    "                print(\"Episode {} Ended | Score {} : {}\".format(i, com_score, my_score))\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 10 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 1 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryBuffer = experience_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "current=set()\n",
    "already=False\n",
    "## set \"results\" as a global variable\n",
    "results=[]\n",
    "s = env.get_states()\n",
    "\n",
    "def on_press(key):\n",
    "    global already\n",
    "    global s\n",
    "    try:\n",
    "        if not key.char in current:\n",
    "            current.add(key.char)\n",
    "    except AttributeError:\n",
    "        if str(key)=='Key.up':\n",
    "            if 'Key.up' in current:\n",
    "                already=True\n",
    "            else:\n",
    "                already=False\n",
    "        \n",
    "        if 'Key.enter' in current and str(key)=='Key.enter':\n",
    "            current.remove('Key.enter')\n",
    "        elif not str(key) in current:\n",
    "            current.add(str(key))\n",
    "\n",
    "    # kk=key)\n",
    "    # kk=current)\n",
    "    # spike\n",
    "    if 'Key.enter' in current:\n",
    "        if 'Key.up' in current:\n",
    "            ## 9: UP & LEFT & SPIKE\n",
    "            if 'Key.left' in current:\n",
    "                kk=9\n",
    "            ## 10: UP & RIGHT & SPIKE\n",
    "            elif 'Key.right' in current:\n",
    "                kk=10\n",
    "            ##  6: UP & SPIKE\n",
    "            else:\n",
    "                kk=6\n",
    "        elif 'Key.down' in current:\n",
    "            ## 11: DOWN & (LEFT | RIGHT) & SPIKE\n",
    "            if 'Key.left' in current or 'Key.right' in current:\n",
    "                kk=11\n",
    "            ##  8: DOWN & SPIKE\n",
    "            else:\n",
    "                kk=8\n",
    "        else:\n",
    "            ## 5: LEFT & SPIKE\n",
    "            if 'Key.left' in current:\n",
    "                kk=5\n",
    "            ## 7: RIGHT & SPIKE\n",
    "            elif 'Key.right' in current:\n",
    "                kk=7\n",
    "            ## 0: SPIKE\n",
    "            else:\n",
    "                kk=0\n",
    "\n",
    "    ## non-spike\n",
    "    else:\n",
    "        ## 1: UP\n",
    "        if 'Key.up' in current and not already:\n",
    "            kk=1\n",
    "        ## 3: LEFT \n",
    "        elif 'Key.left' in current:\n",
    "            kk=3\n",
    "        ## 4: RIGHT\n",
    "        elif 'Key.right' in current:\n",
    "            kk=4\n",
    "        ## 2: NO ACTION\n",
    "        else:\n",
    "            kk=2\n",
    "\n",
    "    s1 = env.get_states()\n",
    "    memoryBuffer.add(np.reshape([s, kk, 0.8, s1, 1], [1,5]))\n",
    "    s = s1\n",
    "\n",
    "def on_release(key):\n",
    "    try:\n",
    "        if key.char in current:\n",
    "            current.remove(key.char)\n",
    "    except AttributeError:\n",
    "        if str(key) in current:\n",
    "            current.remove(str(key))\n",
    "    if key == keyboard.Key.esc:\n",
    "        # Stop listener\n",
    "        return False\n",
    "\n",
    "# Collect events until released\n",
    "with keyboard.Listener(\n",
    "        on_press=on_press,\n",
    "        on_release=on_release) as listener:\n",
    "    s = env.get_states()\n",
    "    listener.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memoryBuffer.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = memoryBuffer.buffer\n",
    "for b in buffer:\n",
    "    b[2]=0.8\n",
    "    memoryBuffer.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1.0 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 8000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 500 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 500 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./dqn\\model-0.ckpt\n",
      "Episode 0 Ended | Score 5 : 1\n",
      "Saved Model\n",
      "Episode 1 Ended | Score 5 : 1\n",
      "Episode 2 Ended | Score 5 : 0\n",
      "Episode 3 Ended | Score 5 : 0\n",
      "Episode 4 Ended | Score 5 : 0\n",
      "Episode 5 Ended | Score 5 : 0\n",
      "Episode 6 Ended | Score 5 : 1\n",
      "Episode 7 Ended | Score 5 : 0\n",
      "Episode 8 Ended | Score 5 : 1\n",
      "Episode 9 Ended | Score 5 : 0\n",
      "570 0.0 0.9921250000000028\n",
      "Episode 10 Ended | Score 5 : 1\n",
      "Saved Model\n",
      "Episode 11 Ended | Score 5 : 0\n",
      "Episode 12 Ended | Score 5 : 0\n",
      "Episode 13 Ended | Score 5 : 0\n",
      "Episode 14 Ended | Score 5 : 0\n",
      "Episode 15 Ended | Score 5 : 0\n",
      "Episode 16 Ended | Score 5 : 0\n",
      "Episode 17 Ended | Score 5 : 0\n",
      "Episode 18 Ended | Score 5 : 0\n",
      "Episode 19 Ended | Score 5 : 0\n",
      "1029 0.0 0.9404875000000212\n",
      "Episode 20 Ended | Score 5 : 0\n",
      "Saved Model\n",
      "Episode 21 Ended | Score 5 : 1\n",
      "Episode 22 Ended | Score 5 : 0\n",
      "Episode 23 Ended | Score 5 : 1\n",
      "Episode 24 Ended | Score 5 : 0\n",
      "Episode 25 Ended | Score 5 : 0\n",
      "Episode 26 Ended | Score 5 : 0\n",
      "Episode 27 Ended | Score 5 : 2\n",
      "Episode 28 Ended | Score 5 : 0\n",
      "Episode 29 Ended | Score 5 : 1\n",
      "1486 0.0 0.8890750000000396\n",
      "Episode 30 Ended | Score 5 : 0\n",
      "Saved Model\n",
      "Episode 31 Ended | Score 5 : 0\n",
      "Episode 32 Ended | Score 5 : 0\n",
      "Episode 33 Ended | Score 5 : 0\n",
      "Episode 34 Ended | Score 5 : 0\n",
      "Episode 35 Ended | Score 5 : 0\n",
      "Episode 36 Ended | Score 5 : 0\n",
      "Episode 37 Ended | Score 5 : 0\n",
      "Episode 38 Ended | Score 5 : 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-63ff00d62163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimageIn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mtotal_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mroundBuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\pika_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, key_num)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# do action and get reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\yBigTa\\Project\\alphachu3\\AlphaChu\\environments\\action.py\u001b[0m in \u001b[0;36msend_key\u001b[1;34m(self, key_num)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mwin32api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeybd_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin32con\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKEYEVENTF_KEYUP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mwin32api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeybd_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin32con\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKEYEVENTF_KEYUP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "myBuffer.add(memoryBuffer.buffer)\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        env.reset_game()\n",
    "        s, _, _, _ = env.step(0)\n",
    "        d = 0\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        # 1 Episode\n",
    "        while True:\n",
    "            roundBuffer = experience_buffer()\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,11)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            s1,r,d, _ = env.step(a)\n",
    "            total_steps += 1\n",
    "            roundBuffer.add(np.reshape(np.array([s,a,0,s1,d]),[1,5]))\n",
    "\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:np.stack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:np.stack(trainBatch[:,0]),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "\n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "\n",
    "            if d==3:\n",
    "                reward = env.get_reward()\n",
    "                rAll += reward\n",
    "                buffer = roundBuffer.buffer\n",
    "                annealed_reward = smoothing(len(buffer), reward)\n",
    "                for idx, x in enumerate(annealed_reward):\n",
    "                    buffer[idx][2] = x\n",
    "                episodeBuffer.add(buffer)\n",
    "                continue\n",
    "\n",
    "            s = s1\n",
    "\n",
    "            if d==4:\n",
    "                com_score, my_score = env.state.get_score()\n",
    "                print(\"Episode {} Ended | Score {} : {}\".format(i, com_score, my_score))\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 10 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
